{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f39d7072",
   "metadata": {},
   "source": [
    "**This cell contains the base code for the Xgboost Regressor model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load CSV data\n",
    "data = pd.read_csv('Sample_Data_5000.csv')\n",
    "\n",
    "# Convert non-numeric values to numeric\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values (if any)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data[['pH', 'EC', 'W_Temp', 'A_Temp', 'Humid', 'CO2']]\n",
    "y = data['Decimal_Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the XGBoost regression model with hyperparameters\n",
    "model = XGBRegressor(\n",
    "    objective='reg:squarederror',  # Use reg:squarederror instead of reg:linear\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=1,\n",
    "    colsample_bytree=1,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Round the predicted values to the closest integers\n",
    "rounded_y_pred = [int(round(val)) for val in y_pred]\n",
    "\n",
    "# Calculate and print the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, rounded_y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98874b",
   "metadata": {},
   "source": [
    "**This cell contains the code to generate the synthetic dataset on which the Xgboost Regressor model is tested**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e0b030",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import csv\n",
    "import random\n",
    "\n",
    "def generate_random_values(num_rows):\n",
    "    rows = []\n",
    "    for _ in range(num_rows):\n",
    "        pH = round(random.uniform(5.4, 6.9), 2)\n",
    "        EC = round(random.uniform(0.3, 1.5), 2)\n",
    "        W_Temp = round(random.uniform(16, 28), 2)\n",
    "        A_Temp = round(random.uniform(16, 28), 2)\n",
    "        Humid = round(random.uniform(50, 95), 1)\n",
    "        CO2 = random.randint(300, 1100)\n",
    "        Light_on_off = random.randint(0, 1)\n",
    "\n",
    "        rows.append([pH, EC, W_Temp, A_Temp, Humid, CO2, Light_on_off])\n",
    "\n",
    "    return rows\n",
    "\n",
    "def apply_conditions(rows):\n",
    "    for row in rows:\n",
    "        # Additional columns\n",
    "        W_Temp = row[2]\n",
    "        A_Temp = row[3]\n",
    "        Humid = row[4]\n",
    "        CO2 = row[5]\n",
    "\n",
    "        row += [1 if W_Temp > 22 else 0,\n",
    "                1 if A_Temp > 24 else 0,\n",
    "                1 if CO2 > 1000 else 0,\n",
    "                1 if Humid > 80 else 0,\n",
    "                1 if Humid < 60 else 0,\n",
    "                1 if row[0] > 6.5 else 0,\n",
    "                1 if row[0] < 5.8 else 0,\n",
    "                1 if row[1] < 0.6 else 0,\n",
    "                1 if row[1] > 1.2 else 0]\n",
    "\n",
    "def convert_to_binary_decimal(rows):\n",
    "    for row in rows:\n",
    "        binary = \"\".join(str(val) for val in row[7:15])  # Exclude Light_on_off column\n",
    "        decimal_label = int(binary, 2)\n",
    "        row += [binary, decimal_label]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_rows = 50000\n",
    "    data_rows = generate_random_values(num_rows)\n",
    "    apply_conditions(data_rows)\n",
    "    convert_to_binary_decimal(data_rows)\n",
    "\n",
    "    with open('Sample_Data_50000.csv', 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['pH', 'EC', 'W_Temp', 'A_Temp', 'Humid', 'CO2', 'Light_on_off',\n",
    "                             'Water_cooler', 'Air_Cooler', 'Air_Vent', 'Dehumidifier',\n",
    "                             'Humidifier', 'Acid_Dozer', 'Base_Dozer', 'Nutrient_Dozer',\n",
    "                             'Distilled_Water_Dozer', 'Binary', 'Decimal_Label'])\n",
    "        csv_writer.writerows(data_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b00e94",
   "metadata": {},
   "source": [
    "**This cell contains the code which performs Cross-Validation on the Xgboost Regressor model to determine the optimal hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a3fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "# Load CSV data\n",
    "data = pd.read_csv('Sample_Data_5000.csv')\n",
    "\n",
    "# Convert non-numeric values to numeric\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values (if any)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data[['pH', 'EC', 'W_Temp', 'A_Temp', 'Humid', 'CO2']]\n",
    "y = data['Decimal_Label']\n",
    "\n",
    "# Create the XGBoost regression model\n",
    "xgb_model = XGBRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],      # Number of boosting rounds\n",
    "    'learning_rate': [0.01, 0.1, 0.2],   # Step size shrinkage\n",
    "    'max_depth': [3, 5, 7],              # Maximum depth of each tree\n",
    "    'min_child_weight': [1, 3, 5],       # Minimum sum of instance weight needed in a child\n",
    "    'gamma': [0, 0.1, 0.2],              # Minimum loss reduction required to make a split\n",
    "    'subsample': [0.8, 1.0],             # Subsample ratio of the training instances\n",
    "    'colsample_bytree': [0.8, 1.0],      # Subsample ratio of columns when constructing a tree\n",
    "    'reg_alpha': [0, 0.1, 0.5],          # L1 regularization term on weights\n",
    "    'reg_lambda': [0, 1, 10]             # L2 regularization term on weights\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=make_scorer(accuracy_score),\n",
    "    n_jobs=-1,  # Use all available cores for parallel computation\n",
    "    verbose=2,  # Show progress during fitting\n",
    "    cv=5        # Number of cross-validation folds\n",
    ")\n",
    "\n",
    "# Perform the grid search to find the best hyperparameters\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters and best accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc28e869",
   "metadata": {},
   "source": [
    "**This cell contains the code which performs K-Fold Cross-Validation on the Xgboost Regressor model to determine the average accuracy of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a99b068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cross-Validation Accuracy:  0.9991\n",
      "CPU times: total: 50.4 s\n",
      "Wall time: 50.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd                # Importing pandas for data manipulation and analysis\n",
    "from xgboost import XGBRegressor   # Importing XGBRegressor from xgboost library for building the XGBoost regression model\n",
    "from xgboost import plot_tree\n",
    "from xgboost import to_graphviz\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score  # Importing functions for data splitting and cross-validation\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_absolute_error  # Importing accuracy_score metric for evaluation (not used in this regression scenario)\n",
    "\n",
    "# Load CSV data\n",
    "data = pd.read_csv('Sample_Data_5000.csv')  # Reading the CSV file and storing the data in a pandas DataFrame 'data'\n",
    "\n",
    "# Convert non-numeric values to numeric\n",
    "data = data.apply(pd.to_numeric, errors='coerce')  # Converting non-numeric values to NaN (Not-a-Number)\n",
    "\n",
    "# Drop rows with NaN values (if any)\n",
    "data.dropna(inplace=True)  # Removing rows containing NaN values from the DataFrame\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data[['pH', 'EC', 'W_Temp', 'A_Temp', 'Humid', 'CO2']]  # Extracting the features (independent variables) from 'data'\n",
    "y = data['Decimal_Label']  # Extracting the target variable (dependent variable) from 'data'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Splitting the data into training and testing sets using 80% for training and 20% for testing.\n",
    "# The random_state is set to 42 to ensure reproducibility.\n",
    "\n",
    "# Train the XGBoost regression model with hyperparameters\n",
    "model = XGBRegressor(\n",
    "    objective='reg:squarederror',  # Objective function to use for regression, 'reg:squarederror' minimizes the mean squared error\n",
    "    n_estimators=1000,  # Number of boosting rounds (trees) to build\n",
    "    learning_rate=0.1,   # Step size shrinkage used to prevent overfitting\n",
    "    max_depth=3,         # Maximum depth of a tree. Increasing it may lead to overfitting.\n",
    "    min_child_weight=1,  # Minimum sum of instance weight (hessian) needed in a child\n",
    "    gamma=0,             # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    subsample=1,         # Subsample ratio of the training instance. Lower values prevent overfitting.\n",
    "    colsample_bytree=1,  # Subsample ratio of columns when constructing each tree. Lower values prevent overfitting.\n",
    "    reg_alpha=0,         # L1 regularization term on weights\n",
    "    reg_lambda=1,        # L2 regularization term on weights\n",
    "    random_state=42      # Seed for random number generator for reproducibility\n",
    ")\n",
    "\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "def k_fold_cross_validation(X, y, model, k=10):\n",
    "    n_samples = len(X)\n",
    "    fold_size = n_samples // k\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(k):\n",
    "        start_idx = i * fold_size\n",
    "        end_idx = (i + 1) * fold_size if i < k - 1 else n_samples\n",
    "\n",
    "        X_test = X[start_idx:end_idx]\n",
    "        y_test = y[start_idx:end_idx]\n",
    "\n",
    "        X_train = np.concatenate((X[:start_idx], X[end_idx:]), axis=0)\n",
    "        y_train = np.concatenate((y[:start_idx], y[end_idx:]), axis=0)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        #y_pred = model.predict(X_test)\n",
    "        y_pred_rounded = model.predict(X_test).round().astype(int)\n",
    "\n",
    "        accuracy = np.mean(y_pred_rounded == y_test)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    return avg_accuracy\n",
    "\n",
    "avg_accuracy = k_fold_cross_validation(X.values, y.values, model)\n",
    "\n",
    "print(\"Average Cross-Validation Accuracy: \", avg_accuracy)\n",
    "# xgb_tree = to_graphviz(mine, num_trees=-1, rankdir='UT')\n",
    "# xgb_tree.view()\n",
    "\n",
    "# plt.figure(figsize=(20, 15), dpi=90)\n",
    "# plot_tree(mine, num_trees=-1, ax=plt.gca())  # You can specify the tree index you want to plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8adad11",
   "metadata": {},
   "source": [
    "**This cell contains the code which tests the model trained above with a new dataset and determines the accuracy with which the model can predict the control actions as well as export the Xgboost Regressor model as a .json file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6696fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on the new dataset: 0.64874\n",
      "R-squared on the new dataset: 0.9998740299246831\n",
      "Mean Absolute Error (MAE) on the new dataset: 0.04042\n",
      "Percentage of Correctly Predicted Control Actions: 98.90%\n",
      "CPU times: total: 1.78 s\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Importing the required libraries\n",
    "import pandas as pd                # Pandas for data manipulation and analysis\n",
    "from xgboost import XGBRegressor   # XGBoost regressor for building the model\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error  # Metrics for evaluation\n",
    "import random  # For random sampling\n",
    "\n",
    "\n",
    "# Load the new dataset\n",
    "new_data = pd.read_csv('Sample_Data_50000.csv')\n",
    "\n",
    "\n",
    "# Preprocess the new dataset\n",
    "# Convert non-numeric values to numeric by replacing them with NaN (Not-a-Number)\n",
    "new_data = new_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with any missing (NaN) values, if any\n",
    "new_data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Split the new dataset into features (X_new) and target variable (y_new)\n",
    "# X_new contains the features (independent variables) and y_new contains the target variable (dependent variable)\n",
    "X_new = new_data[['pH', 'EC', 'W_Temp', 'A_Temp', 'Humid', 'CO2']]\n",
    "y_new = new_data['Decimal_Label']\n",
    "\n",
    "\n",
    "# Make predictions on the new dataset using the trained model\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "\n",
    "# Round the predicted values to the nearest integer\n",
    "y_pred_rounded = [int(round(val)) for val in predictions]\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a counter to keep track of incorrectly predicted data points\n",
    "count = 0\n",
    "\n",
    "\n",
    "# Print the actual and predicted values for the 50 random data points\n",
    "# print(\"Random 50 Data Points - Actual vs. Predicted Values:\")\n",
    "for i in range(len(X_new)):\n",
    "    actual_value = y_new.iloc[i]\n",
    "    predicted_value = y_pred_rounded[i]\n",
    "    if actual_value != predicted_value:\n",
    "        count += 1\n",
    "\n",
    "        \n",
    "# Calculate the mean squared error (MSE) on the new dataset\n",
    "mse = mean_squared_error(y_new, y_pred_rounded)\n",
    "\n",
    "\n",
    "# Optionally, calculate and print other metrics like R-squared and Mean Absolute Error (MAE)\n",
    "r2 = r2_score(y_new, y_pred_rounded)\n",
    "mae = mean_absolute_error(y_new, y_pred_rounded)\n",
    "\n",
    "\n",
    "# Calculate the percentage of correctly predicted control actions\n",
    "percentage_correct = 100 - (count / len(X_new)) * 100\n",
    "\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error (MSE) on the new dataset:\", mse)\n",
    "print(\"R-squared on the new dataset:\", r2)\n",
    "print(\"Mean Absolute Error (MAE) on the new dataset:\", mae)\n",
    "print(f\"Percentage of Correctly Predicted Control Actions: {percentage_correct:.2f}%\")\n",
    "\n",
    "# Saves the Xgboost Regressor model as a .json file which can later be imported to run the model independently\n",
    "model.save_model('xgboost_regressor_model.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254d76da",
   "metadata": {},
   "source": [
    "**This cell contains code to run the Xgboost Regressor model from the .json file such that it can be imported to other devices to run the model independently**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "154b0475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on the new dataset: 0.64874\n",
      "R-squared on the new dataset: 0.9998740299246831\n",
      "Mean Absolute Error (MAE) on the new dataset: 0.04042\n",
      "Percentage of Correctly Predicted Control Actions: 98.90%\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load the trained XGBoost Regressor model\n",
    "loaded_model = XGBRegressor()\n",
    "loaded_model.load_model('xgboost_regressor_model.json')\n",
    "\n",
    "# Load the new dataset from a CSV file (assuming 'Sample_Data_50000.csv' contains your data)\n",
    "new_data = pd.read_csv('Sample_Data_50000.csv')\n",
    "\n",
    "# Preprocess the new dataset by converting non-numeric values to numeric by replacing them with NaN (Not-a-Number)\n",
    "new_data = new_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with any missing (NaN) values, if any\n",
    "new_data.dropna(inplace=True)\n",
    "\n",
    "# Split the new dataset into features (X_new) and target variable (y_new)\n",
    "# X_new contains the features (independent variables) and y_new contains the target variable (dependent variable)\n",
    "X_new = new_data[['pH', 'EC', 'W_Temp', 'A_Temp', 'Humid', 'CO2']]\n",
    "y_new = new_data['Decimal_Label']\n",
    "\n",
    "# Make predictions on the new dataset using the trained model\n",
    "predictions = loaded_model.predict(X_new)\n",
    "\n",
    "# Round the predicted values to the nearest integer\n",
    "y_pred_rounded = [int(round(val)) for val in predictions]\n",
    "\n",
    "\n",
    "# Initialize a counter to keep track of incorrectly predicted data points\n",
    "count = 0\n",
    "\n",
    "# Print the actual and predicted values for the 50 random data points\n",
    "# For idx in random_indices:\n",
    "for i in range(len(X_new)):\n",
    "    actual_value = y_new.iloc[i]\n",
    "    predicted_value = y_pred_rounded[i]\n",
    "    if actual_value != predicted_value:\n",
    "        count += 1\n",
    "\n",
    "# Calculate the mean squared error (MSE) on the new dataset\n",
    "mse = mean_squared_error(y_new, y_pred_rounded)\n",
    "\n",
    "# Optionally, calculate and print other metrics like R-squared and Mean Absolute Error (MAE)\n",
    "r2 = r2_score(y_new, y_pred_rounded)\n",
    "mae = mean_absolute_error(y_new, y_pred_rounded)\n",
    "\n",
    "# Calculate the percentage of correctly predicted control actions\n",
    "percentage_correct = 100 - (count / len(X_new)) * 100\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error (MSE) on the new dataset:\", mse)\n",
    "print(\"R-squared on the new dataset:\", r2)\n",
    "print(\"Mean Absolute Error (MAE) on the new dataset:\", mae)\n",
    "print(f\"Percentage of Correctly Predicted Control Actions: {percentage_correct:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
